{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Regime-Aware Portfolio Optimization (Three Agent Folder Migration)\n",
                "This notebook consolidates `hmm.py` and `Finrlmain.py` for execution in Google Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
                "!pip install yfinance pandas numpy torch matplotlib seaborn hmmlearn gym gymnasium"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import warnings\n",
                "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "from hmmlearn import hmm\n",
                "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
                "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
                "from gym import spaces\n",
                "from gym.utils import seeding\n",
                "from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
                "\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. HMM Regime Detection (from `hmm.py`)\n",
                "Detects market regimes using exogenous features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MarketRegimeHMM:\n",
                "    def __init__(self, n_regimes=4):\n",
                "        self.n_regimes = n_regimes\n",
                "        self.model = hmm.GaussianHMM(\n",
                "            n_components=n_regimes, \n",
                "            covariance_type=\"diag\", \n",
                "            n_iter=1000,\n",
                "            random_state=42\n",
                "        )\n",
                "        self.is_fitted = False\n",
                "\n",
                "    def prepare_data(self, df):\n",
                "        returns_df = df.pivot(index='date', columns='tic', values='close').pct_change().fillna(0)\n",
                "        X = returns_df.values\n",
                "        return X, returns_df.index\n",
                "\n",
                "    def fit(self, df):\n",
                "        X, dates = self.prepare_data(df)\n",
                "        self.model.fit(X)\n",
                "        self.is_fitted = True\n",
                "        state_means = self.model.means_ \n",
                "        avg_returns = state_means.mean(axis=1)\n",
                "        self.sorted_states = np.argsort(avg_returns) \n",
                "        print(f\"HMM fitted. Regime order (Low Return -> High Return): {self.sorted_states}\")\n",
                "        return self\n",
                "\n",
                "    def predict(self, df):\n",
                "        if not self.is_fitted:\n",
                "            raise ValueError(\"HMM not fitted yet.\")\n",
                "        X, dates = self.prepare_data(df)\n",
                "        regimes = self.model.predict(X)\n",
                "        mapped_regimes = np.zeros_like(regimes)\n",
                "        for i, state in enumerate(self.sorted_states):\n",
                "            mapped_regimes[regimes == state] = i\n",
                "        return pd.DataFrame({'date': dates, 'regime': mapped_regimes})\n",
                "\n",
                "    def predict_next_regime(self, current_regime):\n",
                "        if not self.is_fitted:\n",
                "            raise ValueError(\"HMM not fitted yet.\")\n",
                "        raw_state = self.sorted_states[current_regime]\n",
                "        trans_probs = self.model.transmat_[raw_state]\n",
                "        next_raw_state = np.argmax(trans_probs)\n",
                "        next_mapped_regime = np.where(self.sorted_states == next_raw_state)[0][0]\n",
                "        return next_mapped_regime\n",
                "\n",
                "def plot_regimes(df, regime_df, save_path='results/regimes.png'):\n",
                "    os.makedirs('results', exist_ok=True)\n",
                "    returns_df = df.pivot(index='date', columns='tic', values='close').pct_change().dropna()\n",
                "    market_return = returns_df.mean(axis=1)\n",
                "    cum_returns = (1 + market_return).cumprod()\n",
                "    fig, ax = plt.subplots(figsize=(15, 7))\n",
                "    ax.plot(cum_returns.index, cum_returns.values, color='black', alpha=0.3, label='Market Cum. Returns')\n",
                "    regime_colors = ['red', 'orange', 'blue', 'green'] \n",
                "    labels = ['High Bear', 'Sideways/Low Bear', 'Sideways/Low Bull', 'High Bull']\n",
                "    for i in range(len(regime_colors)):\n",
                "        mask = regime_df['regime'] == i\n",
                "        dates = regime_df.loc[mask, 'date']\n",
                "        for d in dates:\n",
                "            ax.axvspan(d, d, color=regime_colors[i], alpha=0.15)\n",
                "    from matplotlib.lines import Line2D\n",
                "    custom_lines = [Line2D([0], [0], color=regime_colors[i], lw=4, alpha=0.5) for i in range(len(regime_colors))]\n",
                "    ax.legend(custom_lines + [Line2D([0], [0], color='black', alpha=0.3)], labels + ['Market'])\n",
                "    plt.title('Market Regimes Detected by HMM')\n",
                "    plt.savefig(save_path)\n",
                "    plt.show()\n",
                "    print(f\"Regime plot saved to {save_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Helper Functions and Environment Overrides (from `Finrlmain.py`)\n",
                "Injects custom logic into FinRL's `StockPortfolioEnv`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def enforce_portfolio_constraints(weights, max_weight=0.60):\n",
                "    weights = np.array(weights).copy()\n",
                "    weights = np.clip(weights, 0, 1)\n",
                "    weights = weights / (weights.sum() + 1e-8)\n",
                "    for _ in range(5):\n",
                "        over = weights > max_weight\n",
                "        if not over.any(): break\n",
                "        excess = weights[over] - max_weight\n",
                "        total_excess = excess.sum()\n",
                "        weights[over] = max_weight\n",
                "        under = weights < max_weight\n",
                "        if under.any():\n",
                "            current_mass = weights[under].sum()\n",
                "            if current_mass > 0:\n",
                "                weights[under] += total_excess * (weights[under] / current_mass)\n",
                "            else:\n",
                "                weights[under] += total_excess / under.sum()\n",
                "    return weights / (weights.sum() + 1e-8)\n",
                "\n",
                "def _init_override(self, df, stock_dim, hmax, initial_amount, transaction_cost_pct, reward_scaling, state_space, action_space, tech_indicator_list, turbulence_threshold=None, lookback=252, day=0, **kwargs):\n",
                "    self.day = day\n",
                "    self.lookback = lookback\n",
                "    self.df = df\n",
                "    self.stock_dim = stock_dim\n",
                "    self.hmax = hmax\n",
                "    self.initial_amount = initial_amount\n",
                "    self.transaction_cost_pct = transaction_cost_pct\n",
                "    self.reward_scaling = reward_scaling\n",
                "    self.state_space = state_space\n",
                "    self.action_space_dim = action_space\n",
                "    self.tech_indicator_list = tech_indicator_list\n",
                "    self.action_space = spaces.Box(low=0, high=1, shape=(self.action_space_dim,), dtype=np.float32)\n",
                "    self.unique_dates = self.df.date.unique()\n",
                "    self.data = self.df[self.df.date == self.unique_dates[self.day]]\n",
                "    self.covs = self.data[\"cov_list\"].iloc[0]\n",
                "    tech_array = np.array([self.data[tech].values for tech in self.tech_indicator_list])\n",
                "    self.regime_df = kwargs.get('regime_df', None)\n",
                "    current_regime = 0\n",
                "    if self.regime_df is not None:\n",
                "        regime_match = self.regime_df.loc[self.regime_df.date == self.unique_dates[self.day], 'future_regime']\n",
                "        if not regime_match.empty: current_regime = regime_match.values[0]\n",
                "    self.state = np.vstack([self.covs, tech_array]).astype(np.float32)\n",
                "    regime_row = np.full((1, self.state.shape[1]), current_regime).astype(np.float32)\n",
                "    self.state = np.vstack([self.state, regime_row])\n",
                "    self.state_memory = [self.state] * self.lookback\n",
                "    self.terminal = False\n",
                "    self.portfolio_value = self.initial_amount\n",
                "    self.asset_memory = [self.initial_amount]\n",
                "    self.portfolio_return_memory = [0]\n",
                "    self.actions_memory = [[1/self.stock_dim]*self.stock_dim]\n",
                "    self.date_memory = [self.unique_dates[self.day]]\n",
                "\n",
                "def _step_override(self, actions):\n",
                "    self.terminal = self.day >= len(self.unique_dates) - 1\n",
                "    if self.terminal:\n",
                "        df = pd.DataFrame(self.portfolio_return_memory, columns=['daily_return'])\n",
                "        if df['daily_return'].std() != 0:\n",
                "            self.sharpe = (252**0.5) * df['daily_return'].mean() / df['daily_return'].std()\n",
                "        return self.state, self.reward, self.terminal, False, {}\n",
                "    else:\n",
                "        weights = enforce_portfolio_constraints(actions)\n",
                "        self.actions_memory.append(weights)\n",
                "        last_day_memory = self.data\n",
                "        self.day += 1\n",
                "        self.data = self.df[self.df.date == self.unique_dates[self.day]]\n",
                "        self.covs = self.data[\"cov_list\"].iloc[0]\n",
                "        tech_array = np.array([self.data[tech].values for tech in self.tech_indicator_list])\n",
                "        self.state = np.vstack([self.covs, tech_array]).astype(np.float32)\n",
                "        current_regime = 0\n",
                "        if self.regime_df is not None:\n",
                "            regime_match = self.regime_df.loc[self.regime_df.date == self.unique_dates[self.day], 'future_regime']\n",
                "            if not regime_match.empty: current_regime = regime_match.values[0]\n",
                "        regime_row = np.full((1, self.state.shape[1]), current_regime).astype(np.float32)\n",
                "        self.state = np.vstack([self.state, regime_row])\n",
                "        self.state_memory.pop(0); self.state_memory.append(self.state)\n",
                "        portfolio_return = sum(((self.data.close.values / last_day_memory.close.values) - 1) * weights)\n",
                "        self.portfolio_return_memory.append(portfolio_return)\n",
                "        self.date_memory.append(self.unique_dates[self.day])\n",
                "        self.portfolio_value *= (1 + portfolio_return)\n",
                "        self.asset_memory.append(self.portfolio_value)\n",
                "        self.reward = portfolio_return * self.reward_scaling\n",
                "        return np.array(self.state_memory), self.reward, self.terminal, False, {}\n",
                "\n",
                "def _reset_override(self, seed=None, options=None):\n",
                "    if seed is not None: self.np_random, seed = seeding.np_random(seed)\n",
                "    self.asset_memory = [self.initial_amount]; self.day = 0\n",
                "    self.data = self.df[self.df.date == self.unique_dates[self.day]]\n",
                "    self.covs = self.data[\"cov_list\"].iloc[0]\n",
                "    tech_array = np.array([self.data[tech].values for tech in self.tech_indicator_list])\n",
                "    self.state = np.vstack([self.covs, tech_array]).astype(np.float32)\n",
                "    current_regime = 0\n",
                "    if self.regime_df is not None:\n",
                "        regime_match = self.regime_df.loc[self.regime_df.date == self.unique_dates[self.day], 'future_regime']\n",
                "        if not regime_match.empty: current_regime = regime_match.values[0]\n",
                "    regime_row = np.full((1, self.state.shape[1]), current_regime).astype(np.float32)\n",
                "    self.state = np.vstack([self.state, regime_row])\n",
                "    self.state_memory = [self.state] * self.lookback\n",
                "    self.portfolio_value = self.initial_amount; self.terminal = False\n",
                "    self.portfolio_return_memory = [0]; self.actions_memory = [[1/self.stock_dim]*self.stock_dim]\n",
                "    self.date_memory = [self.unique_dates[self.day]]\n",
                "    return np.array(self.state_memory), {}\n",
                "\n",
                "StockPortfolioEnv.__init__ = _init_override\n",
                "StockPortfolioEnv.step = _step_override\n",
                "StockPortfolioEnv.reset = _reset_override\n",
                "StockPortfolioEnv._seed = lambda self, seed=None: seeding.np_random(seed)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Pipeline and HMM Fitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TICKER_LIST = ['AAPL','MSFT','NVDA','GOOGL','META','JNJ','UNH','PFE','JPM','BAC','GS','XOM','CVX','WMT','PG','BA','CAT','AMZN', 'AMD', 'NFLX', 'V', 'HD', 'MCD','KO', 'PEP','DIS', 'COST','CRM', 'INTC', 'TXN','GE', 'MMM', 'HON','C', 'GS', 'MS','ABT', 'ABBV', 'MRK']\n",
                "TICKER_LIST = sorted(list(set(TICKER_LIST)))\n",
                "START_DATE = \"2015-01-01\"; END_DATE = \"2024-01-01\"; INITIAL_AMOUNT = 1_000_000\n",
                "\n",
                "print(\"Fetching data...\")\n",
                "df = YahooDownloader(start_date=START_DATE, end_date=END_DATE, ticker_list=TICKER_LIST).fetch_data()\n",
                "fe = FeatureEngineer(use_technical_indicator=True, tech_indicator_list=[\"macd\", \"rsi\", \"cci\", \"adx\"], use_vix=False, use_turbulence=False)\n",
                "df = fe.preprocess_data(df)\n",
                "\n",
                "print(\"Fetching exogenous benchmarks for HMM...\")\n",
                "df_exo = YahooDownloader(start_date=START_DATE, end_date=END_DATE, ticker_list=['SPY', 'DBC', 'LQD', 'EMB', 'TLT', 'TIP']).fetch_data()\n",
                "df_exo = df_exo.sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n",
                "\n",
                "print(\"Cleaning data...\")\n",
                "df = df.drop_duplicates(subset=[\"date\", \"tic\"])\n",
                "counts = df.groupby('tic').size(); max_c = counts.max()\n",
                "df = df[df.tic.isin(counts[counts == max_c].index.tolist())].sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n",
                "TICKER_LIST = sorted(df.tic.unique().tolist())\n",
                "\n",
                "def add_covariance_matrix(df, lookback=20):\n",
                "    df = df.sort_values(['date', 'tic'], ignore_index=True)\n",
                "    df.index = df.date.factorize()[0]\n",
                "    cov_list = []; unique_dates = df.date.unique()\n",
                "    for i in range(lookback, len(unique_dates)):\n",
                "        data_window = df.loc[i - lookback:i, :]\n",
                "        price_pivot = data_window.pivot_table(index='date', columns='tic', values='close')\n",
                "        cov_list.append(price_pivot.pct_change().dropna().cov().values)\n",
                "    df_cov = pd.DataFrame({'date': unique_dates[lookback:], 'cov_list': cov_list})\n",
                "    return df.merge(df_cov, on='date').sort_values(['date', 'tic']).reset_index(drop=True)\n",
                "\n",
                "print(\"Computing covariance matrix...\")\n",
                "df = add_covariance_matrix(df, lookback=20)\n",
                "\n",
                "print(\"Fitting HMM...\")\n",
                "hmm_model = MarketRegimeHMM(n_regimes=4).fit(df_exo)\n",
                "regime_df = hmm_model.predict(df_exo)\n",
                "regime_df['future_regime'] = regime_df['regime'].apply(lambda x: hmm_model.predict_next_regime(x))\n",
                "plot_regimes(df, regime_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Reinforcement Learning (A2C)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Actor(nn.Module):\n",
                "    def __init__(self, input_dim, num_assets, hidden=256):\n",
                "        super().__init__()\n",
                "        self.feature_extractor = nn.Sequential(nn.Linear(input_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden), nn.ReLU())\n",
                "        self.heads = nn.ModuleList([nn.Linear(hidden, num_assets) for _ in range(4)])\n",
                "    def forward(self, x):\n",
                "        regime_indices = x[:, -1].long() \n",
                "        features = self.feature_extractor(x)\n",
                "        out = torch.zeros(x.shape[0], self.heads[0].out_features, device=x.device)\n",
                "        for i in range(4):\n",
                "            mask = (regime_indices == i)\n",
                "            if mask.any(): out[mask] = self.heads[i](features[mask])\n",
                "        return torch.nn.functional.softplus(out) + 1.0\n",
                "\n",
                "class Critic(nn.Module):\n",
                "    def __init__(self, input_dim, hidden=256):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(nn.Linear(input_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n",
                "    def forward(self, x): return self.net(x).squeeze(-1)\n",
                "\n",
                "def train_a2c(env, epochs=100, gamma=0.99, lr=1e-4, value_coef=0.5, entropy_coef=0.01, batch_size=20):\n",
                "    obs_dim = np.prod(env.observation_space.shape); act_dim = env.action_space.shape[0]\n",
                "    actor = Actor(obs_dim, act_dim).to(DEVICE); critic = Critic(obs_dim).to(DEVICE)\n",
                "    optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=lr)\n",
                "    rewards_history = []\n",
                "    for ep in range(epochs):\n",
                "        state, _ = env.reset(); done = False; ep_reward = 0\n",
                "        s_buf, w_buf, r_buf, m_buf = [], [], [], []\n",
                "        while not done:\n",
                "            s_in = torch.tensor(state.flatten(), dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "            with torch.no_grad():\n",
                "                alpha = actor(s_in); dist = torch.distributions.Dirichlet(alpha); weights = dist.sample()\n",
                "            action = weights.cpu().numpy()[0]\n",
                "            next_state, reward, done, _, _ = env.step(action)\n",
                "            s_buf.append(s_in); w_buf.append(weights); r_buf.append(torch.tensor([reward], device=DEVICE)); m_buf.append(torch.tensor([1 - float(done)], device=DEVICE))\n",
                "            state = next_state; ep_reward += reward\n",
                "            if len(r_buf) >= batch_size:\n",
                "                b_s = torch.cat(s_buf); b_w = torch.cat(w_buf); alpha_b = actor(b_s); vals = critic(b_s).squeeze()\n",
                "                dist_b = torch.distributions.Dirichlet(alpha_b); log_probs = dist_b.log_prob(b_w); ents = dist_b.entropy()\n",
                "                with torch.no_grad():\n",
                "                    s_next = torch.tensor(next_state.flatten(), dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "                    nv = critic(s_next) if not done else torch.zeros(1, 1, device=DEVICE)\n",
                "                rets = []; R = nv.squeeze()\n",
                "                for r, m in zip(reversed(r_buf), reversed(m_buf)): R = r + gamma * R * m; rets.insert(0, R)\n",
                "                rets = torch.stack(rets).squeeze(); advs = rets - vals\n",
                "                actor_loss = -(log_probs * advs.detach()).mean(); critic_loss = advs.pow(2).mean()\n",
                "                loss = actor_loss + value_coef * critic_loss - entropy_coef * ents.mean()\n",
                "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
                "                s_buf.pop(0); w_buf.pop(0); r_buf.pop(0); m_buf.pop(0)\n",
                "            if done: s_buf, w_buf, r_buf, m_buf = [], [], [], []\n",
                "        rewards_history.append(ep_reward)\n",
                "        if ep % 10 == 0: print(f\"Episode {ep:03d} | Reward: {ep_reward:.4f}\")\n",
                "    return actor, critic, rewards_history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gym import spaces\n",
                "env = StockPortfolioEnv(df=df, stock_dim=len(TICKER_LIST), hmax=100, initial_amount=INITIAL_AMOUNT, transaction_cost_pct=0.001, reward_scaling=1000.0, state_space=len(TICKER_LIST), action_space=len(TICKER_LIST), tech_indicator_list=[\"macd\", \"rsi\", \"cci\", \"adx\"], lookback=20, day=0, regime_df=regime_df)\n",
                "env.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(env.lookback, env.state.shape[0], env.state.shape[1]), dtype=np.float32)\n",
                "\n",
                "actor, critic, rewards = train_a2c(env, epochs=100)\n",
                "\n",
                "plt.figure(figsize=(10, 6)); plt.plot(rewards); plt.title('A2C Training Progress'); plt.xlabel('Episode'); plt.ylabel('Reward'); plt.show()\n",
                "\n",
                "print(\"\\n[Evaluating...]\")\n",
                "state, _ = env.reset(); done = False; all_w = []\n",
                "while not done:\n",
                "    s = torch.tensor(state.flatten(), dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
                "    with torch.no_grad():\n",
                "        alpha = actor(s); weights = (alpha / alpha.sum(dim=-1, keepdim=True)).cpu().numpy()[0]\n",
                "        weights = enforce_portfolio_constraints(weights); all_w.append(weights)\n",
                "    state, _, done, _, _ = env.step(weights)\n",
                "\n",
                "print(f\"Final Portfolio Value: ${env.portfolio_value:,.2f}\")\n",
                "print(f\"Total Return: {(env.portfolio_value / INITIAL_AMOUNT - 1) * 100:.2f}%\")\n",
                "\n",
                "print(\"\\nFinal Portfolio Allocation:\")\n",
                "if len(all_w) > 0:\n",
                "    last_weights = all_w[-1]\n",
                "    for i, ticker in enumerate(TICKER_LIST):\n",
                "        print(f\"  {ticker:5s}: {last_weights[i]*100:6.2f}%\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}